# -*- coding: utf-8 -*-
"""ANN PISTACHIO CLASSIFICATION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tPXzPVfFrSnSm-zUcHLZEIJNkR24UQyf
"""

# standard libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
warnings.simplefilter('ignore')
import gc

# keras libraries
import tensorflow
from tensorflow import keras
from keras import models
from keras import layers
from keras import metrics
from keras.metrics import Precision
from tqdm.keras import TqdmCallback
from keras.backend import clear_session

# sklearn libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import RobustScaler,MinMaxScaler,StandardScaler

# visualisation
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

dataset = pd.read_excel("/content/Pistachio_28_Features_Dataset (1).xlsx")

# drop NULLs
dataset.dropna(inplace=True)

# encoding the target
le = LabelEncoder()
dataset.Class = le.fit_transform(dataset['Class']) 

# print details
print(f"There are {dataset.shape[0]} records with {dataset.Class.nunique()} classes")
print(f"Classes are: {', '.join(x for x in list(le.classes_))}")

# define features and target
X = dataset.drop('Class',axis=1)
y = dataset[['Class']]

# converting all the features (which are not) to float 
int_cols = X.select_dtypes(include=np.int_).columns.tolist()
for c in int_cols:
    X[c] = X[c].astype('float')
    
# split training & test data [80-20 split]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=np.random.randint(100))

# Feature selection
mutual_information = mutual_info_classif(X_train, y_train, n_neighbors=5, copy = True)

plt.subplots(1, figsize=(28, 1))
sns.color_palette("Set2")
sns.heatmap(mutual_information[:, np.newaxis].T, cmap=sns.color_palette("ch:start=.2,rot=-.3", as_cmap=True), cbar=False, linewidths=1, annot=True, annot_kws={"size": 10})
plt.yticks([], [])
plt.gca().set_xticklabels(X.columns, rotation=25, ha='right', fontsize=11)
plt.suptitle("Feature Importance", fontsize=12, y=1.2)
plt.gcf().subplots_adjust(wspace=0.4)

# new feature list
imp_cols = ['Eccentricity', 'Roundness', 'Aspect_Ratio', 'Compactness', 'Major_Axis']
X_new = X[imp_cols]

# scaling the features
rs = RobustScaler()
X_scaled = rs.fit_transform(X_new)

# split training & test data [80-20 split] -- again
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=np.random.randint(100))

# record stats
print(f"Training Records = {X_train.shape[0]} ; Validation Record {X_test.shape[0]} ")

# define model params
input_shape = X_train.shape[1]                                                         
output_shape = y_train.shape[1]                                                           
nodes = np.ceil(2 * input_shape / 3 + output_shape).astype(int)                         
batch_size = np.ceil(len(X_train) / 128).astype(int)                                     

# build model
model = models.Sequential()
# input layer
model.add(layers.Dense(
                        batch_size
                       ,input_shape=(input_shape,)
                       ,name='input_layer'
                       ,activation='selu'))
# hidden layer
model.add(layers.Dense(
                        nodes
                        ,name='hidden_nides'
                       ,activation='selu'))
# dropout layer
model.add(layers.Dropout(0.5))

# output layer
model.add(layers.Dense(
                        output_shape
                        ,name='output_layer'
                       ,activation='sigmoid'))

# compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# summmary
model.summary()

# model fit params
epoch = 200

# garbage collect, incase we're re-running the model to get the best fit
gc.collect()
    
# reset model , incase we're re-running the model to get the best fit
clear_session()

# fit the keras model on the dataset
hist = model.fit(X_train
                  ,y_train
                  ,epochs = epoch
                  ,batch_size=batch_size
                  ,shuffle=True
                  ,validation_data=(X_test, y_test)
                  ,verbose=0
                  ,callbacks=[TqdmCallback(verbose=0)]
          )

acc = '{:.2%}'.format(hist.history['accuracy'][-1])
print(f"The model has achieved an accuracy of {acc} with {epoch} epochs")

# visualize training results
acc = hist.history['accuracy']
val_acc = hist.history['val_accuracy']

loss = hist.history['loss']
val_loss = hist.history['val_loss']

epochs_range = range(epoch)

plt.figure(figsize=(20, 10))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Predictions 
y_pred = model.predict(X_test)
y_pred_class = [round(x[0]) for x in y_pred]
y_test_class = y_test.reset_index(drop=True)

# confusion matrix
cfm = confusion_matrix(y_test_class, y_pred_class)

# visualise confusion matrix
plt.figure(figsize=(8,8))
group_names = ['True Neg','False Pos','False Neg','True Pos']
group_counts = ["{0:0.0f}".format(value) for value in cfm.flatten()]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_names,group_counts)]
sns.heatmap(cfm, annot=True, cmap='crest', cbar=False, fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# classification report
class_names = []
for i in y['Class'].unique():
    class_names.append(le.inverse_transform([i])[0])

print(classification_report(y_test_class, y_pred_class, target_names=class_names))